Chapter 13. Concurrency


Concurrency is a decoupling strategy. It helps us decouple what gets done from when it
gets done. In single-threaded applications what and when are strongly coupled

Decoupling what from when can dramatically improve both the throughput and struc-
tures of an application. From a structural point of view the application looks like many lit-
tle collaborating computers rather than one big main loop. This can make the system easier
to understand and offers some powerful ways to separate concerns.

1. Myths and Misconceptions

  If you aren’t very careful, you can create some very nasty situations.
  Consider these common myths and misconceptions:

  - Concurrency always improves performance:
    
    Concurrency can sometimes improve performance, but only when there is a lot of wait
    time that can be shared between multiple threads or multiple processors. Neither situ-
    ation is trivial.

  - Design does not change when writing concurrent programs:

    In fact, the design of a concurrent algorithm can be remarkably different from the
    design of a single-threaded system. The decoupling of what from when usually has a
    huge effect on the structure of the system.

  - Understanding concurrency issues is not important when working with a container
    such as a Web or EJB container:

    In fact, you’d better know just what your container is doing and how to guard against
    the issues of concurrent update and deadlock


2. Concurrency Defense principles.


  What follows is a series of principles and techniques for defending your systems from the
  problems of concurrent code

  - Single Responsibility Principle

    Concurrency design is complex enough to be a reason to change in it’s own right
    and therefore deserves to be separated from the rest of the code. Unfortunately, it is all too
    common for concurrency implementation details to be embedded directly into other pro-
    duction code. Here are a few things to consider:

    * Concurrency-related code has its own life cycle of development, change, and tuning.

    * Concurrency-related code has its own challenges, which are different from and often
      more difficult than nonconcurrency-related code.

    * The number of ways in which miswritten concurrency-based code can fail makes it
      challenging enough without the added burden of surrounding application code.

    Recommendation: Keep your concurrency-related code separate from other code.

  
  - Limit the scope of data
    
    two threads modifying the same field of a shared object can interfere with each
    other, causing unexpected behavior. One solution is to use the synchronized keyword to
    protect a critical section in the code that uses the shared object. It is important to restrict
    the number of such critical sections. The more places shared data can get updated, the
    more likely:

    * You will forget to protect one or more of those places

    * There will be duplication of effort required to make sure everything is effectively
      guarded (violation of DRY)

    * It will be difficult to determine the source of failures, which are already hard enough
      to find.

      Recommendation: Take data encapsulation to heart; severely limit the access of any
      data that may be shared.
    
  - Use copies of data

    A good way to avoid shared data is to avoid sharing the data in the first place. In some situations 
    it is possible to copy objects and treat them as read-only. In other cases it might be
    possible to copy objects, collect results from multiple threads in these copies and then
    merge the results in a single thread.

    
  - Threads Should Be as Independent as Possible

    Consider writing your threaded code such that each thread exists in its own world, sharing
    no data with any other thread. Each thread processes one client request, with all of its
    required data coming from an unshared source and stored as local variables. This makes
    each of those threads behave as if it were the only thread in the world and there were no
    synchronization requirements.

    Recommendation: Attempt to partition data into independent subsets than can be
    operated on by independent threads, possibly in different processors.


  - Know Your Executions Models

    * Producer - Consumer:

      One or more producer threads create some work and place it in a buffer or queue. One or
      more consumer threads acquire that work from the queue and complete it. The queue
      between the producers and consumers is a bound resource. This means producers must
      wait for free space in the queue before writing and consumers must wait until there is
      something in the queue to consume. Coordination between the producers and consumers
      via the queue involves producers and consumers signaling each other. The producers write
      to the queue and signal that the queue is no longer empty. Consumers read from the queue
      and signal that the queue is no longer full. Both potentially wait to be notified when they
      can continue.

    * Readers - Writers:

      When you have a shared resource that primarily serves as a source of information for readers, 
      but which is occasionally updated by writers, throughput is an issue. Emphasizing
      throughput can cause starvation and the accumulation of stale information. Allowing
      updates can impact throughput. Coordinating readers so they do not read something a
      writer is updating and vice versa is a tough balancing act. Writers tend to block many read-
      ers for a long period of time, thus causing throughput issues.
      The challenge is to balance the needs of both readers and writers to satisfy correct
      operation, provide reasonable throughput and avoiding starvation. A simple strategy
      makes writers wait until there are no readers before allowing the writer to perform an
      update. If there are continuous readers, however, the writers will be starved. On the other
      hand, if there are frequent writers and they are given priority, throughput will suffer. Find-
      ing that balance and avoiding concurrent update issues is what the problem addresses.

    * Dining Philosophers:

      Imagine a number of philosophers sitting around a circular table. A fork is placed to the
      left of each philosopher. There is a big bowl of spaghetti in the center of the table. The
      philosophers spend their time thinking unless they get hungry. Once hungry, they pick
      up the forks on either side of them and eat. A philosopher cannot eat unless he is holding
      two forks. If the philosopher to his right or left is already using one of the forks he
      needs, he must wait until that philosopher finishes eating and puts the forks back down.
      Once a philosopher eats, he puts both his forks back down on the table and waits until he
      is hungry again.
      Replace philosophers with threads and forks with resources and this problem is simi-
      lar to many enterprise applications in which processes compete for resources. Unless care-
      fully designed, systems that compete in this way can experience deadlock, livelock,
      throughput, and efficiency degradation.

    Most concurrent problems you will likely encounter will be some variation of these
    three problems. Study these algorithms and write solutions using them on your own so
    that when you come across concurrent problems, you’ll be more prepared to solve the
    problem.
    Recommendation: Learn these basic algorithms and understand their solutions.


  - Beware Dependencies Between Synchronized Methods

    Dependencies between synchronized methods cause subtle bugs in concurrent code. The
    Java language has the notion of synchronized , which protects an individual method. How-
    ever, if there is more than one synchronized method on the same shared class, then your
    system may be written incorrectly. 12
    Recommendation: Avoid using more than one method on a shared object.
    There will be times when you must use more than one method on a shared object.
    When this is the case, there are three ways to make the code correct:

    • Client-Based Locking—Have the client lock the server before calling the first
      method and make sure the lock’s extent includes code calling the last method.
    
    • Server-Based Locking—Within the server create a method that locks the server, calls
      all the methods, and then unlocks. Have the client call the new method.

    • Adapted Server—create an intermediary that performs the locking. This is an exam-
      ple of server-based locking, where the original server cannot be changed.

  - Keep Synchronized Sections Small

    The synchronized keyword introduces a lock. All sections of code guarded by the
    same lock are guaranteed to have only one thread executing through them at any given
    time. Locks are expensive because they create delays and add overhead. So we don’t
    want to litter our code with synchronized statements. On the other hand, critical sec-
    tions must be guarded. So we want to design our code with as few critical sections as
    possible.        
  

  - Writing Correct Shut-Down Code Is Hard

    Graceful shutdown can be hard to get correct. Common problems involve deadlock,
    with threads waiting for a signal to continue that never comes.
    For example, imagine a system with a parent thread that spawns several child threads
    and then waits for them all to finish before it releases its resources and shuts down. What if
    one of the spawned threads is deadlocked? The parent will wait forever, and the system
    will never shut down.

    Situations like this are not at all uncommon. So if you must write concurrent code that
    involves shutting down gracefully, expect to spend much of your time getting the shut-
    down to happen correctly.
    Recommendation: Think about shut-down early and get it working early. It’s going to
    take longer than you expect. Review existing algorithms because this is probably harder
    than you think.


  - Testing Threaded Code

    Proving that code is correct is impractical. Testing does not guarantee correctness. How-
    ever, good testing can minimize risk. This is all true in a single-threaded solution. As soon
    as there are two or more threads using the same code and working with shared data, things
    get substantially more complex.

    Recommendation: Write tests that have the potential to expose problems and then
    run them frequently, with different programatic configurations and system configurations
    and load. If tests ever fail, track down the failure. Don’t ignore a failure just because the
    tests pass on a subsequent run.

    * Treat spurious failures as candidate threading issues: 
    
      Threaded code causes things to fail that “simply cannot fail.” Most developers do not have
      an intuitive feel for how threading interacts with other code (authors included). Bugs in
      threaded code might exhibit their symptoms once in a thousand, or a million, executions.
      Attempts to repeat the systems can be frustratingly. This often leads developers to write off
      the failure as a cosmic ray, a hardware glitch, or some other kind of “one-off.” It is best to
      assume that one-offs do not exist. The longer these “one-offs” are ignored, the more code
      is built on top of a potentially faulty approach.
      Recommendation: Do not ignore system failures as one-offs.

    
    * Get your nonthreaded code working first:

      Make sure code works outside
      of its use in threads. Generally, this means creating POJOs that are called by your threads.
      The POJOs are not thread aware, and can therefore be tested outside of the threaded envi-
      ronment. The more of your system you can place in such POJOs, the better.
      Recommendation: Do not try to chase down nonthreading bugs and threading bugs
      at the same time. Make sure your code works outside of threads.

    * Make your threaded code pluggable:

      Write the concurrency-supporting code such that it can be run in several configurations:
      • One thread, several threads, varied as it executes
      • Threaded code interacts with something that can be both real or a test double.
      • Execute with test doubles that run quickly, slowly, variable.
      • Configure tests so they can run for a number of iterations.
      Recommendation: Make your thread-based code especially pluggable so that you
      can run it in various configurations.

    * Make your threaded code tunable:

      Getting the right balance of threads typically requires trial an error. Early on, find ways to
      time the performance of your system under different configurations. Allow the number of  
      threads to be easily tuned. Consider allowing it to change while the system is running.
      Consider allowing self-tuning based on throughput and system utilization.    

    * Run with more threads than processors:
      
      Things happen when the system switches between tasks. To encourage task swapping, run
      with more threads than processors or cores. The more frequently your tasks swap, the more
      likely you’ll encounter code that is missing a critical section or causes deadlock.
      
    * Run on different platforms: 

      different operating systems have different threading policies, each of which impacts
      the code’s execution. Multithreaded code behaves differently in different environments. 16
      You should run your tests in every potential deployment environment.
      Recommendation: Run your threaded code on all target platforms early and often.
      
    * Instrument your code to try and force failures:

      It is normal for flaws in concurrent code to hide. Simple tests often don’t expose them.
      Indeed, they often hide during normal processing. They might show up once every few
      hours, or days, or weeks!

      The reason that threading bugs can be infrequent, sporadic, and hard to repeat, is that
      only a very few pathways out of the many thousands of possible pathways through a vul-
      nerable section actually fail. So the probability that a failing pathway is taken can be star-
      tlingly low. This makes detection and debugging very difficult.
      How might you increase your chances of catching such rare occurrences? You can
      instrument your code and force it to run in different orderings by adding calls to methods
      like Object.wait() , Object.sleep() , Object.yield() and Object.priority() .
      Each of these methods can affect the order of execution, thereby increasing the odds
      of detecting a flaw. It’s better when broken code fails as early and as often as possible.
      There are two options for code instrumentation:
      
      • Hand-coded
      • Automated

      -> hand coded: 
          
        You can insert calls to wait() , sleep() , yield() , and priority() in your code by hand. It
        might be just the thing to do when you’re testing a particularly thorny piece of code.

        There are many problems with this approach:
        • You have to manually find appropriate places to do this.
        • How do you know where to put the call and what kind of call to use?
        • Leaving such code in a production environment unnecessarily slows the code down.
        • It’s a shotgun approach. You may or may not find flaws. Indeed, the odds aren’t with you.

      -> Automated:

        Consider:

        public class ThreadJigglePoint {
          public static void jiggle() {
          }
        }

        You can add calls to this in various places within your code:
        public synchronized String nextUrlOrNull() {
          if(hasNext()) {
            ThreadJiglePoint.jiggle();
            String url = urlGenerator.next();
            ThreadJiglePoint.jiggle();
            updateHasNext();
            ThreadJiglePoint.jiggle();
            return url;
          }
          return null;
        }

        Now you use a simple aspect that randomly selects among doing nothing, sleeping, or 
        yielding.

        Or imagine that the ThreadJigglePoint class has two implementations. The first imple-
        ments jiggle to do nothing and is used in production. The second generates a random
        number to choose between sleeping, yielding, or just falling through.

        Though a bit simplistic, this could be a reasonable option in lieu of a more sophisticated tool.
        
Conclusion

  Concurrent code is difficult to get right. Code that is simple to follow can become night-
  marish when multiple threads and shared data get into the mix. If you are faced with writ-
  ing concurrent code, you need to write clean code with rigor or else face subtle and
  infrequent failures.

  First and foremost, follow the Single Responsibility Principle. Break your system into
  POJOs that separate thread-aware code from thread-ignorant code. Make sure when you
  are testing your thread-aware code, you are only testing it and nothing else. This suggests
  that your thread-aware code should be small and focused.

  Know the possible sources of concurrency issues: multiple threads operating on
  shared data, or using a common resource pool. Boundary cases, such as shutting down
  cleanly or finishing the iteration of a loop, can be especially thorny.

  Learn your library and know the fundamental algorithms. Understand how some of
  the features offered by the library support solving problems similar to the fundamental
  algorithms.

  Learn how to find regions of code that must be locked and lock them. Do not lock
  regions of code that do not need to be locked. Avoid calling one locked section from
  another. This requires a deep understanding of whether something is or is not shared. Keep
  the amount of shared objects and the scope of the sharing as narrow as possible. Change
  designs of the objects with shared data to accommodate clients rather than forcing clients
  to manage shared state.

  Issues will crop up. The ones that do not crop up early are often written off as a one-
  time occurrence. These so-called one-offs typically only happen under load or at seem-
  ingly random times. Therefore, you need to be able to run your thread-related code in
  many configurations on many platforms repeatedly and continuously. Testability, which
  comes naturally from following the Three Laws of TDD, implies some level of plug-ability,
  which offers the support necessary to run code in a wider range of configurations.

  You will greatly improve your chances of finding erroneous code if you take the time
  to instrument your code. You can either do so by hand or using some kind of automated
  technology. Invest in this early. You want to be running your thread-based code as long as
  possible before you put it into production.
  If you take a clean approach, your chances of getting it right increase drastically.



1. Test-selection strategies

  To Select the next test We need to rely on our intuition and a variety of
  heuristics when thinking about the multiple properties of the candidate tests. 
  To start with, we might want to consider the following four strategies to 
  aid our decision of which test to write next:
  
  - Diving first into details versus starting with the big picture:

    Details first is a good way of tackling risks such as, “Are we able to recognize
    a pattern from a user-submitted image?” but solving the pattern recognition 
    problem delays progress (certainty) regarding the big picture. Big-picture first, 
    on the other hand, is a good way of testing out the envisioned design quickly 
    without needing to worry about the details. The downside of big-picture first 
    is that we’re delaying progress on the details.

  - Exploring uncertain territory versus sticking with the familiar and comfortable

    Reducing uncertainty is one factor we might consider when selecting the next
    test. The alternative for reducing uncertainty would be to stick with the familiar
    and proceed on a safe, known path for the moment, dealing with the uncertain
    later. The advantage of exploring the uncertain is simple and straightforward—
    we’re effectively reducing risks by turning the unknown into the known.

    Although there is value in reducing risk, sometimes there’s even more value in
    quickly beating down a known path and picking the low-hanging fruit first,
    instead of climbing the tree right away to figure out whether we can do it.
  
  - Going for the highest value versus picking the low-hanging fruit first

    Looking at a list of tests we have yet to write, we can usually identify a couple of
    tests that are clearly more work than others. Similarly, we can usually identify dif-
    ferences in how valuable the behavior represented by the test in question is for
    the larger whole. Ideally, we should try to identify and select tests that let us make
    the most progress (yield the most value) with the smallest effort. Most of the time,
    however, there’s no such clear candidate, and we need to decide among multiple
    tests with more or less equal benefit-to-effort price tags.

    A typical (if somewhat exaggerated) example of a high-value test versus a low-
    hanging fruit might be testing an object for positive functionality versus testing
    the same object for handling null input. The positive functionality is likely more
    laborious to implement but almost by definition yields more value. The defensive
    functionality, handling null input, is likely far easier to implement but also yields
    much, much less value in terms of functionality.

  - Treading the happy path first versus starting with handling error situations.

    As a rule of thumb, I drive for the happy path first before proceeding to handle
    exceptional scenarios, such as exceptions thrown by third-party API s, malformed
    input, and so forth. The reason for doing so is mostly about value. Even the most
    robust system capable of handling any conceivable error situation is practically
    useless when it isn’t able to perform its basic function. On the other hand, even a
    system that, for example, is prone to crash if the network is temporarily down still
    provides value as long as the network isn’t down.

    but every once in a while we might face a situation where it makes sense to test-drive 
    by leaving the happy path for last, first going through all error situations.

  Don’t think too much about which test is the “right” one to write next.
  There’s no right sequence in the first place, and these test-selection strategies will
  become second nature over time.


2. Making your test pass

  there are many ways to. There are three ways to make progress: 
  faking it, triangulation, and obvious implementation.

  - Faking it:

    Sometimes after writing a failing test, we don’t have an idea how to correctly
    implement the code that makes the test pass. In such cases, we might want to
    resort to faking the particular functionality so that we get back to green.
    Faking it is better than staying in the red for a long time.

    As it happens, the easiest way to fake something is often to return hard-coded results

    After we’ve faked something, it’s easy to switch into triangulation mode, mainly
    because we know there’s that one tiny snippet of clearly non-production-quality
    code in our code base that we need to somehow smoke out and replace with a real
    implementation.

  - Triangulation:

    When we wanted to squeeze out a hard-coded string from our production code. 
    Triangulation is a useful technique for evolving toward a correct implementation 
    when we don’t yet have a clear concept of how to generalize a given piece of 
    hard-coded logic.

    In the triangulation technique we TDD folks use, each of the tests we write 
    constrains the available solution space to one dimension. When we have enough tests, 
    the tests effectively narrow down—triangulate—the exact solution we were looking for.

    For example, in developing a credit-card validation component, we might not
    know how to structure our class hierarchy for supporting multiple credit cards
    with their slightly differing number lengths and validation rules. Using triangula-
    tion, we might first write tests and implement the code for validating just one
    brand of cards, say, MasterCard. These tests would be easy to write, and the code
    to satisfy those tests would be just as straightforward.
    Next, we’d pick another card vendor, say, Visa, and we’d write tests to verify
    that our component handles Visa as well as MasterCard, distinguishing between
    them where necessary. These new tests would slowly start pushing our code
    toward a state where the suitable places for generalization become obvious. By the
    time we’re adding a test for Diners Club, it’s likely that we’ll see where the code
    wants us to generalize.

  - Obvious implementation

    Luckily, most of the time the necessary step to make a test pass is an obvious one.
    We don’t mean obvious as in hard coding something, but rather as the “correct”
    implementation being obvious. In such situations, we can simply proceed with
    what we consider an obvious implementation, taking a slightly bigger step than we
    usually take when triangulating or faking our way towards the ultimate goal.

3. Prime Guidelines for test-driving

  - Do. Not. Skip. Refactoring:

    Not refactoring mercilessly and leaving duplication in the code base is about
    the closest thing to attaching a time bomb to your chair. Unfortunately, we are
    good at remembering the “test” and “code” steps of the TDD cycle and extremely
    proficient at neglecting a code smell that screams for the missing step.

    pay attention to not skipping refactoring. Crosscheck with your programming 
    buddy to spot any duplication you may have missed.

  - Get to green fast:

    We don’t, however, go for the simplest design right off the
    bat in the code step. Instead, we should strive to get back to green fast. 
    The code step is where we get to that green bar with as few edits as possible. 
    The refactoring step is where we perfect our design.

  - Slow down after a mistake:

    It is common for developers practicing TDD to start taking slightly bigger and 
    bigger steps as time goes by. At some point, however, we’ll take too big a bite 
    off our test list and end up reverting our changes. At these points, 
    we should realize that the steps we’re taking are too big compared to our 
    ability to understand the needed changes to our implementation. 
    We need to realize that we must tighten our play. Small steps.


4. Essential testing concepts.

  - Fixures:

    a fixture is the shared state from which all the different test methods of a test 
    class begin their execution. Fixtures are more than just what we create in the 
    setup method.
    To summarize the essence of fixtures, they’re the whole shebang—the state of
    the whole runtime environment—rather than just the instance variables of a test
    class instance and the internal state of those objects.

    => fixures remove duplication

    Fixtures move the shared state for multiple tests into one place, 
    effectively removing duplication.

    Although removing all duplication is definitely a good rule of thumb, 
    sometimes it’s better to leave some duplication in our test code for the 
    sake of readability.

    a fixture that’s built by each test method from scratch with
    zero commonalities in setup—is definitely an anti-pattern and should be reme-
    died. Either there’s too much duplication between the tests or there’s no cohe-
    sion between them, the latter of which is a clear indicator that the test class should
    be split into two or more classes.

    => fixures allow for focused tests

    A key to being able to have a focused test is to have a suitable fixture that 
    brings the system and the objects involved to a state in which the test doesn’t 
    need to do much more than a single assertion call, possibly after first invoking 
    a one-liner on the object being tested.

    We don’t waste any of our precious time wading through code, trying to figure out
    where “it” happens. When our fixture is built well, the fixture tells us the context
    and the test goes straight to the point.

    try to keep focused tests that communicate intent effectively, rather than complex 
    tests that are so unfathomable that a week later we can’t tell whether the test 
    is correct or not.


  -Test Doubles

    Sometimes one of our difficulties in testing a given class or object have to do
    with their collaborators and other kinds of dependencies. (Objects that are difficult)
    to create.

    Test doubles are objects that stand in for the real thing. They pretend to be some-
    thing they’re not, but they do it in a way that the client has no idea, and they typically
    do it faster than the real thing—often in terms of both execution time and time
    spent writing and maintaining the test. A typical use of test doubles in our unit tests
    is to first create the test double (or multiple, depending on your needs), then con-
    figure state and/or behavior and/or expectations for the test double, pass the test
    double to the code you’re testing, and verify the results.

    => state based testing

      State-based testing refers to the use of the fixture objects’ 
      (the objects that make up the test fixture) internal state after 
      invoking the class under test in order to verify the correct behavior. 
      We verify the correctness by comparing the primary (class under test) 
      and the collaborator objects’ state to what we expect them to be.

      In some cases, when the fixture is small and doesn’t need much setup, state-
      based testing is a good way to use test doubles, especially if we can use a readily
      available stub implementation for the required interface instead of writing our
      own. In some cases, however, we might want to test for interactions rather than
      changes in the objects’ states.

    => testing for interactions

      Interaction-based testing takes a vastly different approach to verifying correct
      behavior. Instead of asserting that the ending states of the object under test and
      its collaborators match what we expect, interaction-based tests verify that the
      object under test interacted with its collaborators as we expected.

      In other words, we are not interested in the internal state of the collaborators, 
      but we are interested in whether the object being tested made the expected method 
      calls with the expected parameters and, if applicable, in the expected sequence.

      Interaction-based tests are made possible by what we call dynamic mock object
      libraries.

      What these libraries do is let us point to an interface (or a class) and specify
      the expected collaborations (method calls); the library then gives us a test double
      that implements that interface. We can then pass on the test double, which we
      generally refer to as a mock object, to our code under test. After executing the class
      under test, we can ask the mock object to verify that its expectations were indeed
      met.

      remember: when the interactions become long and complex, it’s time to refactor.

      We lean on interaction-based testing to verify how an object talks to its 
      collaborators; and we lean on state-based testing to verify how well the
      object listens.

5. Tests Doubles

  As we just learned, test doubles are substitutable, alternative implementations of
  an interface or class that we don’t want to use in a test—the reason often being
  that the real thing has one of these issues:
  
  - It’s too slow.
  - It’s not available (or doesn’t exist).
  - It depends on something that’s not available (or doesn’t exist).
  - It’s too difficult to instantiate and configure for a test.

  Further examples of issues with regard to testability of objects could include 
  nondeterministic behavior, such as primary-key generation or time-dependent 
  functionality. Additionally, it is often difficult to cause exceptions when using
  real objects in tests (for example, try to think of a way to unplug and plug the 
  network cable from your unit test in order to cause a temporary network error).

  There are, however, more than one type of test double that we can differentiate 
  using the simple classification scheme of stubs, fakes, and mocks. 

  - Stubs: 
      Stubs are essentially the simplest possible implementation of a given interface
      you can think of. For example, stubs’ methods typically return hardcoded, mean-
      ingless values.

  - Fakes: 
      Fakes are a degree more sophisticated than stubs in that they can be 
      considered an alternative implementation of the interface. In other words, 
      a fake looks like a duck and walks like a duck even though it isn’t a real
      duck. In contrast, a stub only looks like a duck.

  - Mocks: 
      Mocks can be considered even more sophisticated in terms of their 
      implementation, because they incorporate assertions for verifying 
      expected collaboration with other objects during a test. 
      Depending on the implementation of a mock, it can be set up either to 
      return hardcoded values or to provide a fake implementation of the logic. 
      Mocks are typically generated dynamically with frameworks and libraries, 
      such as EasyMock, but they can also be implemented by hand.

      mock example =>

        @Test
        public void testOrderProcessorWithEasyMock()
          // arrange
          float initialBalance = 100.0f;
          float listPrice = 30.0f;
          float discount = 10.0f;
          float expectedBalance =
              initialBalance - (listPrice * (1 - discount / 100));
          Customer customer = new Customer(initialBalance);
          Product product = new Product("TDD in Action", listPrice);

          // record expected collaboration with mock PricingService
          PricingService mock = createMock(PricingService.class);
          expect(mock.getDiscountPercentage(customer, product)).andReturn(discount);
          replay(mock);

          // act
          OrderProcessor processor = new OrderProcessor();
          processor.setPricingService(mock);
          processor.process(new Order(customer, product));

          // assert
          assertEquals(expectedBalance, customer.getBalance(), 0.001f);
          verify(mock);
          
6. Guidelines for testable design

  When we test-drive code, we’re making design decisions, and those decisions
  have a direct effect on how easy it is for us to work with that code in the future.
  Writing the tests before the code does indeed enforce a certain level of testabil-
  ity, but it helps if we’re aware of what kinds of designs promote testability and
  what kinds of designs might work now but will likely blow up soon after.

  Aiming at helping you avoid creating testability challenges for yourself, here
  are a few simple design guidelines that we should pay attention to:
  
  - Choose composition over inheritance:

    Although inheritance it's a nice feature for a programming language, 
    inheritance does have its down side when it comes to testability, maintainability,
    and overall flexibility of design.

    Specifically, having to deal with an inheritance hierarchy can sometimes make it 
    unnecessarily difficult to instantiate our objects in a test harness. 
    In Java, for example, we might have to provide valid input only needed by the 
    superclass constructor even though we’re just interested in an aspect 
    of the child class.

    Furthermore, even the smallest of changes could potentially cause a ripple
    effect throughout the class hierarchy, which is obviously not an ideal situation.

    Composition is a way to build objects that provide complex functionality by com-
    bining a set of less complex component objects. The top-level composite object del-
    egates work to its components instead of invoking methods from its superclass. 
    In essence, composition is based on object-level division of responsibility instead of
    static, class-level division.

    Composition tends to be slightly more verbose than
    inheritance, measured in lines of code, but its improved characteristics in terms of
    testability, flexibility, and maintainability often more than outweigh the cost of
    those extra lines of code.

  - Avoid static and the Singleton.

    Depending on how intertwined your par-
    ticular static/Singleton is in the code base you’re testing, it might be surprisingly
    difficult or awkward to replace the implementation behind the static method with
    a test double for your test.

    Static method calls are difficult to fake because the target class for the call is
    hardcoded. Similarly, it is difficult to substitute a fake implementation for a Singleton class
    obtained through the usual (static...) getInstance() method call.

    In fact, it’s not the Singleton pattern itself that causes trouble—it’s 
    the default implementation of the Singleton.

    In most cases, however, I’d recommend first looking at ways to get rid of the static
    methods by converting them into instance methods. In many cases, and with
    object instantiation being dirt cheap with modern virtual machines, that’s a ques-
    tion of simply removing the static keyword and replacing static references with
    instance method calls.

    public class OrderProcessor {
      public void process(Order order) {
        PricingService service = PricingService.getInstance();
        // use the PricingService object for processing the order
      }
    }

    This is bad form. We’re mixing the acquisition of the dependencies with the logic
    that makes use of those dependencies.

  - Isolate dependencies:

    In order to facilitate replacement of dependencies with test doubles, it is essential
    to first isolate the dependencies in a way that lets us perform the replacement as
    easily as possible.

    public class OrderProcessor {
      public void process(Order order) {
        PricingService service = getPricingService(); // this is called a seam
        // use the PricingService object for processing the order
      }
      
      // this can can be overridden to return test double
      // this is called an enabling point
      protected PricingService getPricingService() {
        return PricingService.getInstance();
      }
    }

    The test will look like
    @Test
    public void testOrderProcessorByExploitingTheSeam() {
      OrderProcessor p = new OrderProcessor() {
        protected PricingService getPricingService() {
          return new FakePricingService();
        }
      };
    ...
    }


  - Inject dependencies.

    It is an approach to structuring code in a way that reduces direct dependencies to
    indirect dependencies or, to put it another way, replaces the getter with a setter.

    public class OrderProcessor {

    //  Store dependencies as instance variables
    private PricingService pricingService;
   
      /**
      * Hand me my dependency by calling this method.
      * (Let someone else give dependencies)
      */
      public void setPricingService(PricingService pricingService) {
        this.pricingService = pricingService;
      }
    
      /**
      * Please call setPricingService() before invoking me. Thanks.
      */
      public void process(Order order) {
        //Use dependency directly
        float price = pricingService.getDiscountedPrice(order)
      }
    }

    The test will look like

    @Test
    public void testOrderProcessorWithDependencyInjection() {
      OrderProcessor p = new OrderProcessor();
      p.setPricingService(new FakePricingService());
    }

    Constructor based injection can be considered a better ap-
    proach than setter-based injection because with setter-based injection, the pro-
    grammer is expected to take responsibility for knowing the order in which the
    dependencies should be injected, Thus leaving the possibility of having 
    half-configured objects in the wild.


7. Unit Testing patterns

    - Patterns for writing assertions


      * Resulting State Assertion:

        The Resulting State Assertion pattern is the single most common way to perform
        assertions in a unit test. The fundamental idea is to exercise some functionality 
        on the fixture objects and afterward assert that the internal state of the fixture
        objects match our expectations.

        @Test
        public void sizeOfListReflectsItemsAddedToIt() throws Exception {
          List<String> list = new ArrayList<String>();
          list.add("something");
          assertEquals(1, list.size()); // state verification
        }

        the problem is that the previous test would pass with an implementation 
        of size that always returns 1

        In some cases, however, it’s worth it to not just assert the after
        state but also the before state, which is where the Guard Assertion 
        pattern comes into play.


      * Guard Assertion:

        The Guard Assertion pattern is about making explicit the assumptions made
        about the fixture right before invoking the functionality we want to test.

        @Test
        public void listIsNoLongerEmptyAfterAddingAnItemToIt()
        throws Exception {
          List<String> list = new ArrayList<String>();
          assertTrue(list.isEmpty());
          // guard assertion
          list.add("something");
          assertFalse(list.isEmpty()); // state verification
        }

        Notice how the Guard Assertion ensures that the list’s isEmpty method correctly
        returns true for an empty list before we invoke the add method—which we’re
        really testing with this test.

        The Guard Assertion pattern is a common accomplice of the Resulting State
        Assertion pattern. That is, the two are often combined into a sequence where the
        test first asserts that the before state matches the test author’s expectations and
        only then proceeds to invoke the functionality and assert against the resulting
        state.

        Sometimes, however, the purpose of adding a Guard Assertion is to make sure
        that an assumption about the fixture’s starting state is correct. In these cases, it
        may make sense to move the Guard Assertion(s) into the end of the setup
        method. After all, the setup method is what those assertions are testing.


      * Delta Assertion

        Sometimes we may need to work with code that we don’t have full control over in
        our tests. Specifically, our tests may have a fixture we’re not in control of. 
        This yields the problem of how to write reliable, robust, self-checking tests 
        when we cannot hard code the fixture’s state. The solution is to not assert 
        against the absolute state after invoking the code under test, but rather 
        to test that the difference—or delta—between the initial and after states 
        is what we expect.

        public class TestAddingToArrayList {
          private ArrayList<String> list;
          // setup method omitted for brevity

          @Test
          public void sizeOfListReflectsItemsAddedToIt() throws Exception {
            int sizeBefore = list.size();
            // record the "before" state
            list.add("something");
            assertEquals(sizeBefore + 1, list.size());
            // delta verification
          }
        }

        recording the state before exercising the code under test in order to be able
        to express the final assertion relative to the before state.

        Delta Assertions also have the advantage over regular Resulting State Assertions 
        in that the test is more focused on the essence of what it’s testing rather than 
        throwing seemingly arbitrary values at the poor co-worker trying to make sense 
        of why our test expects the list to contain some magic number of items.

      * Custom Assertion:

        Sometimes the amount of code verifying our expectations vastly exceeds the
        amount of code required to invoke the code under test. When that happens 
        (and especially if it happens more than once), it’s often a good idea to
        extract a Custom Assertion method from the test in order to encapsulate 
        complex verification logic behind a nice little method we can call from 
        the test.

      * Interaction Assertion:

        Interaction Assertions are funny things, in that they don’t verify that our 
        code produces the correct results. Instead, they verify that our code 
        interacts with its collaborator objects as we expect it to.
    

    - Patterns for writing Fixures

      Creating hordes of objects in a massive
      setup method that can’t fit on one screen is bad. 
      Complex, massive code is a problem whether it’s in production code or test code.

      * Parameterized Creation Method

        A lot of the objects in a typical fixture are so-called entity objects in 
        that they represent something that exists in the business domain either as 
        a concrete thing or as a virtual concept. These types of objects generally 
        have a lot of attributes, and most of the code in the bad fixtures I see is 
        about populating those attributes - even though the vast majority of them 
        are of no importance to the test at hand.

        The Parameterized Creation Method attacks this situation by hiding the non-
        important attributes from the setup by extracting the object creation into a separate
        creation method, which takes the variable attributes as arguments and populates
        the non-important ones with constant or random values.

        alice = new Person();
        alice.setId(1L);
        alice.setFirstname("Alice");
        alice.setLastname("Adams");
        alice.setSsn("111111");

        billy = new Person();
        billy.setId(2L);
        billy.setFirstname("Billy");
        billy.setLastname("Burke");
        billy.setSsn("222222");

        replace with => 

        // only the important attributes for this test are provided
        clark = createPerson("Clark", "Cable");
        billy = createPerson("Billy", "Burke");


      * Object Mother

        At first, we can do a lot of good by refactoring our test classes to use local 
        creation methods instead of duplicating code. Soon, we’re likely to find ourselves
        duplicating the creation methods between different test classes. The next natural
        step in removing that duplication is to move the creation methods into a separate
        helper class. The Object Mother pattern describes just such an aggregate of cre-
        ation methods.

        mother object is an object or a set of objects that does the following:
        - Provides a fully-formed business object along with all of its required 
          attribute objects
        - Returns the requested object at any point in its lifecycle
        - Facilitates customization of the delivered object
        - Allows for updating of the object during the testing process
        - When required, terminates the object and all its related objects at the end
          of the test process

        Object Mother might provide methods for modifying the given domain objects by, 
        for example, attaching objects to each other, removing relationships, 
        or moving the given objects to a desired state.

        Although it's a powerful tool and catalyst to motivate test writing, Object
        Mothers take time to build. As such, I would recommend taking small steps refac-
        toring your current test code base toward having cohesive creation methods and
        eventually refactor the creation methods and their canned test data into one or
        more Object Mothers.
        
      * Automated Teardown

        Examples of such necessary cleanup might be integration tests cleaning up 
        created objects from a database or wiping generated files from a file system.

        When the teardown logic is complex or there are simply a lot of objects 
        to tear down, our tests become cluttered and we can easily skip cleaning up 
        something accidentally. And that can create problems down the road that are 
        extremely hard to debug and trace back to the source.

        The Automated Teardown pattern tackles this problem by encapsulating the
        teardown logic into a separate class and bringing the responsibility for 
        triggering that logic right next to where the objects are created in the 
        first place.

8. General tests patterns

  - Parameterized Test

    Every now and then, we find ourselves writing almost identical tests—tests where
    only a few input values are different but the logic is essentially the same. In
    those situations, we might want to consider turning our test class into a Parame-
    terized Test.

    The fundamental idea is that there’s one and only one test method, which encap-
    sulates the test logic to be applied to the parameterized data. There obviously also
    needs to be a method that provides the parameterized data, and there needs to be
    some code to bind the provided data to invocations to the test method.

  - Self-Shunt

    it is kind of a test double-except that it’s our test class. The Self-Shunt pattern, 
    is one where the test class instance acts as a test double in its own tests.

    When a the fake grows in size, so does the degree of cluttering, driving us 
    toward having a Self-Shunt or an actual, named test-double class.

    full-blown test doubles have their own disadvantages, too. If we’d like to share 
    objects and states between our test methods and the test double,
    an Intimate Inner Class might be a better option.

    public class SelfShuntExample implements PricingService {
      @Override
      public float getDiscountPercentage(Customer c, Product p) {
        return 10.0f;
      }
      
      @Test
      public void testOrderProcessorWithMockObject() throws Exception {
      // some setup omitted for brevity...
        OrderProcessor processor = new OrderProcessor();
        processor.setPricingService(this);
        processor.process(new Order(customer, product));
        assertEquals(expectedBalance, customer.getBalance(),
        0.001f);
      }
    }

  - Intimate Inner Class

    There are occasions when we’d like to share some objects between the test class
    and a test double. This can be fulfilled with a getter method but it exposes the
    internals of a test double.

    the ability of a non-static inner class to access
    and assign to the test class’s fields can yield nice, compact test code compared to,
    for example, exposing the internals of a test double through getter methods.

    public class IntimateAnonymousInnerClassExample {
      
      // this is shared between test class and test double
      private StartStopSynchronizedThread thread;
      
      @Test
      public void testStartingAndStoppingThreadsThroughAnExecutorService()
        throws Exception 
      {
        Server server = new Server();
        server.setThreadFactory(new ThreadFactory() {
          // Intimate Inner Class
          public Thread newThread(Runnable task) {
            thread = new StartStopSynchronizedThread(task);
            return thread;
          }
        });

        server.start();
        // tests can access shared fields
        thread.shouldBeStartedWithin(1, TimeUnit.SECONDS);
        server.stop();
        // tests can access shared fields
        thread.shouldBeStoppedWithin(1, TimeUnit.SECONDS);      
      });

    - Privileged Access

      You may have been in a situation where you needed to change that one bit of legacy
      code in order to test your stuff—but there was some reason you couldn’t touch the
      legacy code. In such situations, it may be a reasonable workaround to invade the
      legacy code’s privacy and directly tweak its internals through the Reflection API in
      order to make it possible for us to write tests—Privileged Access, if you will.

      example => 
        Inject.staticField("foo").of(LegacyCode.class).with(ourTestDouble);

      The ability to inject or read private fields of classes or objects isn’t too
      useful for projects that have been using TDD from the get-go, but for those 
      working with mounds of legacy code not designed with testability in mind,
      these kinds of tricks and utilities can save the day. However, if we can change 
      the legacy code we should prefer doing that instead of falling back to some 
      Reflection trickery.

    - Extra constructor

      The world of computing is full of monolithic code bases that look like a spaghetti
      monster. When working in these code bases, it’s not uncommon to have a head-
      ache because we can’t seem to instantiate some class without pulling in a dozen
      other classes along the way. The problem, of course, is that the dependencies of
      the class we’d like to use haven’t been properly isolated. The proper solution
      would be to isolate the dependencies, perhaps by driving the architecture toward
      dependency injection, but that might take time. The Extra Constructor pattern,
      however, provides us with a nice temporary solution.

      The basic idea is that because we cannot substitute the dependencies from the
      outside, we need to do it from the inside. Thus, we add a new constructor to the
      class we want to test—one that takes as arguments the dependencies we’d like to
      substitute. This new constructor can then first delegate to the real constructor
      and subsequently store the passed-in test-double implementations in place of the
      real dependencies.

    - Test-Specific Subclass

      Another common anti-pattern I’ve seen a lot during my programming career is
      the violation of the Single Responsibility Principle

      The Test-Specific Subclass pattern lets us expose or modify internal state or
      behavior in some scenarios were the SRP is violated. 
      for example, we might create a behavior-modifying subclass of the given class
      and override some method to always return some hardcoded value, 
      because we are no interested in what that method does in particular.


9. Working with legacy code

  Legacy code is traditionally considered to mean code written by someone else some-
  where at some point in time. Old code, that is. Old code with a bad stench! The
  source for that bad stench is not the fact that the code is old but that it’s difficult
  to read and possibly buggy, and there’s no way of telling whether changing one
  thing here will break stuff elsewhere

  legacy code = code without tests

  the vast majority of us are not working on green-field projects but rather
  maintaining, developing further, and integrating with systems that have been and
  will be running in production for an arbitrary number of months, years, or even
  decades. This means that we must be able to use TDD in the context of an existing
  code base that wasn’t developed test-first from the beginning.

  - Test driving legacy development

    The way to develop test-first in the context of legacy code is similar to what we
    need to do when others on our team or project don’t write tests first or at all. We
    begin by writing tests around the code we want to change. That might require first
    breaking some dependencies without having tests to watch our back, which means
    proceeding more slowly than usual. That shouldn’t discourage you, however,
    because breaking those dependencies and getting in those tests will pay back the
    investment sooner than you think.

    the process of working with legacy code looks like this

    1. identify change points
    2. identify inflection point
    3. cover the inflection
    3a. break external dependencies
    3b. break internal dependencies
    3c. write tests
    4. make changes
    5. refactor covered code.

    The process described could be split into three main phases:

    1. Analizing the change

      When we start analyzing the change we want to make, we first identify the change
      points. Change points are the places in the code base where we need to edit code
      in order to implement the desired change.

      When we know where the change should take place, we identify the inflection
      point. The inflection point (or test point) is the point “downstream” in our code
      where we can detect any relevant change in the system’s behavior after touching
      the code in the change points. Typical inflection points are close-proximity seams
      such as method invocations after or around the change points

      Close-proximity inflection points typically cover the change point tightly, making
      it easy to test for the current behavior in detail.

      More distant inflection points by definition cover more ground and, thus,
      may protect us from side effects of the change that our analysis had missed

      Close-proximity inflection points tend to provide a more localized checkpoint
      without too much noise around the signal. Distant inflection points, on the other
      hand, are more likely to catch side effects our analysis hadn’t found—but in
      exchange for potentially more effort in writing the tests because we often don’t
      have access to the kind of detailed information we usually have when testing close
      to the change point.

    2. Preparing for the change

      Once we’ve spotted the change and inflection points, we proceed to cover the
      inflection point with tests that nail down the current behavior before we make our
      change. This might involve breaking dependencies with careful edits that expose
      the dependency through a seam we can manipulate in our tests.
      The tests we write to cover the inflection point are typically what we call charac-
      terization tests, meaning that they nail down the current functionality as is, without
      worrying about whether that behavior is correct. Characterization tests are often
      also learning tests in the sense that we use them to verify assumptions we’ve made
      while identifying the change points.

    3. Test-driving the change

      After we’ve written tests around the change and inflection points to the degree
      that we’re comfortable with our test coverage, we make the change by adding a
      test for the desired new functionality. As we proceed with implementing the
      change, our characterization tests tell us if we broke anything while editing the
      legacy code, and the newly added tests tell us when we have managed to imple-
      ment the desired change correctly. Finally, once we’ve successfully made the
      change and all tests are passing, we refactor the code as usual, enjoying the cover
      of our automated tests.
      That’s all there is to working with legacy code in general. The main differences
      between the regular test-driven development cycle and the process described are
      that we need to write tests for the existing behavior before adding a test for the
      new behavior and that we often need to make small dependency-breaking edits
      without our safety net in order to be able to start writing those tests. It just
      requires a bit more care and thought.